# -*- coding: utf-8 -*-
"""Breed_Classification using cnn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x0Jyzg-8p1nqv-58nR1LSyxfgCyTLUoD
"""

# =====================================================
# STEP 1: IMPORT LIBRARIES
# =====================================================
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from keras.applications.resnet50 import ResNet50, preprocess_input
from keras.layers import Dense, GlobalAveragePooling2D, Dropout
from keras.models import Model
from sklearn.metrics import classification_report

import zipfile
from google.colab import files

from google.colab import drive
drive.mount('/content/drive')

# =====================================================
# STEP 2: DIRECTLY UNZIP FROM GOOGLE DRIVE
# =====================================================
from google.colab import drive
drive.mount('/content/drive')

zip_path = "/content/drive/MyDrive/Indian_bovine_breeds.zip"   # ðŸ‘ˆ put your zip path here
extract_path = "/content/dataset"

import zipfile
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print("Dataset extracted to:", extract_path)

# =====================================================
# STEP 3: IMPORT LIBRARIES
# =====================================================
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from keras.applications.resnet50 import ResNet50, preprocess_input
from keras.layers import Dense, GlobalAveragePooling2D, Dropout
from keras.models import Model
from sklearn.metrics import classification_report

# =====================================================
# STEP 4: DATA LOADING (TRAIN / VAL SPLIT)
# =====================================================
data_dir = "/content/dataset"   # where zip was extracted
img_size = 224
batch_size = 32

train_gen = ImageDataGenerator(
    preprocessing_function=preprocess_input,
    validation_split=0.2,
    horizontal_flip=True,
    rotation_range=20,
    zoom_range=0.2
)

train_data = train_gen.flow_from_directory(
    data_dir,
    target_size=(img_size, img_size),
    batch_size=batch_size,
    class_mode='categorical',
    subset='training'
)

val_data = train_gen.flow_from_directory(
    data_dir,
    target_size=(img_size, img_size),
    batch_size=batch_size,
    class_mode='categorical',
    subset='validation'
)

# =====================================================
# STEP 5: RESNET50 MODEL
# =====================================================
base_model = ResNet50(
    weights="imagenet",
    include_top=False,
    input_shape=(img_size, img_size, 3)
)

# Freeze pretrained layers
for layer in base_model.layers:
    layer.trainable = False

# Custom classification layers
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(512, activation='relu')(x)
x = Dropout(0.4)(x)
preds = Dense(train_data.num_classes, activation='softmax')(x)

model = Model(inputs=base_model.input, outputs=preds)

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()

# =====================================================
# STEP 6: TRAIN MODEL
# =====================================================
history = model.fit(
    train_data,
    epochs=15,
    validation_data=val_data
)

# =====================================================
# FINE-TUNED RESNET50 FOR HIGH ACCURACY
# =====================================================

import tensorflow as tf
from keras.applications.resnet50 import ResNet50, preprocess_input
from keras.layers import Dense, GlobalAveragePooling2D, Dropout
from keras.models import Model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
import numpy as np
from sklearn.utils.class_weight import compute_class_weight

# ------------ DATA LOADING ------------
data_dir = "/content/dataset"
img_size = 224
batch_size = 32

train_gen = ImageDataGenerator(
    preprocessing_function=preprocess_input,
    validation_split=0.2,
    rotation_range=25,
    zoom_range=0.25,
    shear_range=0.15,
    width_shift_range=0.2,
    height_shift_range=0.2,
    brightness_range=[0.6, 1.4],
    horizontal_flip=True
)

train_data = train_gen.flow_from_directory(
    data_dir,
    target_size=(img_size,img_size),
    batch_size=batch_size,
    subset="training",
    class_mode="categorical"
)

val_data = train_gen.flow_from_directory(
    data_dir,
    target_size=(img_size,img_size),
    batch_size=batch_size,
    subset="validation",
    class_mode="categorical"
)

# ------------ CLASS WEIGHTS ------------
labels = train_data.classes
class_weights = compute_class_weight(
    class_weight="balanced",
    classes=np.unique(labels),
    y=labels
)
class_weights = dict(enumerate(class_weights))

# ------------ BASE MODEL --------------
base_model = ResNet50(include_top=False, weights="imagenet", input_shape=(224,224,3))

# Freeze only first 120 layers (fine-tune last 60)
for layer in base_model.layers[:120]:
    layer.trainable = False

for layer in base_model.layers[120:]:
    layer.trainable = True

# ------------ CUSTOM LAYERS ------------
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(512, activation="relu")(x)
x = Dropout(0.4)(x)
output = Dense(train_data.num_classes, activation="softmax")(x)

model = Model(inputs=base_model.input, outputs=output)

# ------------ OPTIMIZER + SMOOTHING ------------
loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1)

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
    loss=loss,
    metrics=["accuracy"]
)

model.summary()

# ------------ LR SCHEDULER ------------
lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.3,
    patience=3,
    min_lr=1e-7
)

# ------------ TRAIN MODEL ------------
history = model.fit(
    train_data,
    epochs=30,
    validation_data=val_data,
    class_weight=class_weights,
    callbacks=[lr_scheduler]
)

# =====================================================
# STEP 7: GRAPHS
# =====================================================
plt.figure(figsize=(12,5))

# Accuracy plot
plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label="Train Accuracy")
plt.plot(history.history['val_accuracy'], label="Val Accuracy")
plt.title("Accuracy")
plt.legend()

# Loss plot
plt.subplot(1,2,2)
plt.plot(history.history['loss'], label="Train Loss")
plt.plot(history.history['val_loss'], label="Val Loss")
plt.title("Loss")
plt.legend()

plt.show()

# =====================================================
# STEP 8: CLASSIFICATION REPORT (F1 / PRECISION)
# =====================================================
val_data.reset()
pred = model.predict(val_data)
pred_classes = np.argmax(pred, axis=1)
true_classes = val_data.classes

labels = list(val_data.class_indices.keys())

print("\nClassification Report:")
print(classification_report(true_classes, pred_classes, target_names=labels))

# =====================================================
# STEP 9: SAVE MODEL INTO GOOGLE DRIVE
# =====================================================
model_save_path = "/content/drive/MyDrive/breed_resnet50_model.h5"
model.save(model_save_path)

print("Model saved to Drive at:", model_save_path)